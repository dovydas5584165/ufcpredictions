!pip install optuna
!pip install imbalanced-learn

import pandas as pd
import numpy as np
import joblib
import optuna
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.impute import KNNImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score, classification_report
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier

# Load data
df = pd.read_csv('large_dataset.csv').dropna(subset=['winner'])

# Undersampling to balance the classes
rus = RandomUnderSampler(random_state=42)
df_balanced, _ = rus.fit_resample(df, df['winner'])  # Apply undersampling

# Feature Engineering (Vectorized)
df_balanced = df_balanced.assign(
    age_diff=df_balanced['r_age'] - df_balanced['b_age'],
    reach_diff=df_balanced['r_reach'] - df_balanced['b_reach'],
    rev_diff=df_balanced['r_rev'] - df_balanced['b_rev'],
    sig_str_acc_diff=df_balanced['r_sig_str_acc'] - df_balanced['b_sig_str_acc'],
    td_acc_diff=df_balanced['r_td_acc'] - df_balanced['b_td_acc'],
    sub_attempt_diff=df_balanced['r_sub_att'] - df_balanced['b_sub_att'],
    target=df_balanced['winner'].map({'Red': 1, 'Blue': 0})
)

features = ['reach_diff', 'age_diff', 'rev_diff', 'sig_str_acc_diff', 'td_acc_diff', 'sub_attempt_diff']

# Data Splitting
def split_data(df):
    return {
        'Women': df[df['gender'] == 'Women'],
        'Tiny': df.query("r_weight < 80 & b_weight < 80"),
        'Big Guy': df.query("r_weight >= 80 & b_weight >= 80")
    }

splits = split_data(df_balanced)
models = {}

# Optuna Objective Function
def objective(trial, X, y):
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    f1_scores = []

    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 400),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),
        'random_state': 42,
        'eval_metric': 'logloss'
    }

    for train_idx, val_idx in skf.split(X, y):
        model = XGBClassifier(**params)
        model.fit(X[train_idx], y[train_idx])
        f1_scores.append(f1_score(y[val_idx], model.predict(X[val_idx])))

    return np.mean(f1_scores)

# Model Training Function
def train_model(group, df_group):
    if len(df_group) < 5:
        print(f"Skipping {group} due to low sample size.")
        return

    print(f"Training model for {group}...")

    X, y = df_group[features].values, df_group['target'].values
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

    # Imputation (Precompute for Efficiency)
    imputer = KNNImputer(n_neighbors=5).fit(X_train)
    X_train, X_test = imputer.transform(X_train), imputer.transform(X_test)

    # Balance Data Using SMOTE
    X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)

    # Hyperparameter Optimization (Parallel Execution)
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=30, n_jobs=-1)

    best_params = study.best_params
    print(f"Best Params for {group}: {best_params}")

    # Train Final Model with Optimized Parameters
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', XGBClassifier(**best_params, random_state=42, eval_metric='logloss'))
    ])

    pipeline.fit(X_train, y_train)
    models[group] = pipeline

    # Save pipeline and imputer
    joblib.dump(pipeline, f'model_pipeline_{group}.pkl')
    joblib.dump(imputer, f'imputer_{group}.pkl')

    # Evaluation
    y_pred = pipeline.predict(X_test)
    print(f"F1 Score for {group}: {f1_score(y_test, y_pred):.4f}")
    print(classification_report(y_test, y_pred))

# Train Models for Each Group
for group, df_group in splits.items():
    train_model(group, df_group)

print("Models trained and saved successfully.")
